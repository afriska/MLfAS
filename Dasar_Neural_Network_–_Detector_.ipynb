{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lapisan paling sederhana pada Neural Network disebut juga fully connected layer. Jika terdapat input berupa array “x” satu dimensi dengan panjang in_features dan output berupa array “y” satu dimensi dengan panjang out_features, maka fungsinya berupa perkalian matriks ditambah dengan bias, diikuti fungsi non-linear yang disebut fungsi aktivasi. Linear layer menghitung fungsi berikut.\n",
        "\n",
        "y = x . A^T + b\n",
        "\n",
        "Keterangan:\n",
        "\n",
        "A^T adalah matriks transpose\n",
        "\n",
        "x adalah koefisien dari matriks transpose A^T\n",
        "\n",
        "b adalah sebuah array yang disebut bias\n",
        "\n",
        "x dan b disebut bobot atau weight\n",
        "\n",
        "Linear layer tersebut disebut sebagai fully connected layer karena matriks A menghubungkan setiap elemen input (fitur) ke setiap elemen output. Input dan output  berupa array 1 dimensi. Jika input-nya berupa image, maka harus di-reshape ke bentuk array 1 dimensi.\n",
        "\n",
        "Neural Network sering digunakan sebagai detektor. Dalam kasus ini, setiap nilai dari array output y akan sesuai dengan keluaran satu detektor dan setiap detektor mewakili 1 class.\n",
        "\n",
        "Linear layer biasanya diikuti oleh fungsi aktivasi. Berikut adalah macam – macam fungsi aktivasi.\n",
        "\n",
        "\tFungsi aktivasi ReLU (Rectified Linear Unit)\n",
        "Fungsi aktivasi ReLU didefinisikan dengan rumus sebagai berikut.\n",
        "\n",
        "ReLU(x)=max⁡(0, x)\n",
        "\n",
        "Fungsi aktivasi ReLU membatasi output ke dalam rentang non-negatif. Hal ini berguna ketika output merepresentasikan sebuah detektor. Dalam hal ini nilai 1 dapat berarti “terdeteksi dengan pasti”, nilai 0 berarti “tidak terdeteksi dengan pasti”, dan nilai negatif tidak memiliki makna. Fungsi aktivasi ReLU adalah fungsi aktivasi yang paling sederhana dan paling sering digunakan. Meskipun demikian, fungsi aktivasi ReLU dapat menyebabkan masalah vanishing gradient pada nilai negatif.\n",
        "\n",
        "\tFungsi aktivasi LeakyReLU\n",
        "Fungsi aktivasi LeakyReLU didefinisikan dengan rumus sebagai berikut.\n",
        "\n",
        "LeakyReLU = max⁡(0, x) + negativeSlope × min⁡(0, x)\n",
        "\n",
        "Fungsi aktivasi LeakyReLU merupakan modifikasi dari fungsi aktivasi ReLU untuk menghindari masalah vanishing gradient. Fungsinya memiliki slope yang kecil untuk nilai negatif.\n",
        "\n",
        "\tFungsi aktivasi Softmax\n",
        "Fungsi aktivasi Softmax didefinisikan dengan rumus sebagai berikut.\n",
        "\n",
        "Soft max⁡├(x_i┤) = ⍁{e_i^x}{∑_j^{ }├(e_j^x┤)}\n",
        "\n",
        "Fungsi aktivasi softmax mengubah output dari suatu jaringan agar tampak seperti fungsi probabilitas, yaitu bernilai positif dan menjumlahkannya menjadi 1.\n",
        "\n",
        "\tFungsi aktivasi Sigmoid\n",
        "Fungsi aktivasi Sigmoid didefinisikan dengan rumus sebagai berikut.\n",
        "Sigmoid(x)=1/(1+e^(-x) )\n",
        "\n",
        "Fungsi aktivasi sigmoid diferensial di setiap titik, tetapi lebih kompleks dalam penghitungannya. Fungsi aktivasi ini adalah fungsi aktivasi klasik yang sudah digunakan dalam karya – karya awal neural network.\n",
        "\n",
        "Python Example for a Linear Layer\n",
        "Untuk linear layer, bentuk tensor input dan output adalah sebagai berikut.\n",
        "\n",
        "\tInput: (N, *, Hin) dimana * berarti sejumlah dimensi tambahan dan Hin = in_features.\n",
        "\tOutput: (N, *, Hout) dimana semua kecuali dimensi terakhir memiliki bentuk yang sama dengan input dan Hout = out_features\n",
        "Keterangan:\n",
        "N adalah ukuran batch\n",
        "\n",
        "\\* biasanya adalah indeks untuk test set\n",
        "\n",
        "Hin adalah panjang dari sinyal input\n",
        "\n",
        "Hout adalah jumlah class (detektor) yang diinginkan\n",
        "\n",
        "Indeks batch digunkaan untuk training set dan data target yang berbeda. Pelatihan dilakukan satu batch demi satu batch agar dapat menghemat memori karena hanya satu batch yang dimuat ke dalam memori pada satu waktu.\n",
        "\n",
        "Indeks batch digunkaan untuk training set dan data target yang berbeda. Pelatihan dilakukan satu batch demi satu batch agar dapat menghemat memori karena hanya satu batch yang dimuat ke dalam memori pada satu waktu.\n",
        "\n",
        "Loss function yang sederhana dan banyak digunakan adalah MSE (Mean Squared Error) loss. Selain MSE, jenis loss function lainnya adalah cross-entropy loss yang biasa digunakan untuk detektor. Jenis loss function ini mencipatakan jarak antara dua distribusi probabilitas. Distribusi probabilitas adalah distribusi target (probabilitas 1 untuk kelas true dan probabilitas 0 untuk kelas false) dan distribusi yang dihasilkan oleh neural network  (probabilitas prediksi untuk setiap kelas, dengan nilai probabilitas yang tinggi untuk kelas yang jaringan anggap ada pada inputan-nya.\n",
        "\n",
        "Untuk mendapatkan solusi yang lebih baik dan menyertakan loss function lainnya, umumnya optimasi numerik digunakan untuk menemukan bobot koefisien yang meminimalkan loss function yang diberikan. Loss function menghasilkan satu angka atau nilai untuk training set dan bobot yang diberikan. Untuk mengurangi loss function, bobot akan diperbaharui selama proses optimasi. Optimizer yang umum adalah SGD (Stochastic Gradient Descent) dan Adam (Adaptive Moments).\n",
        "\n",
        "Setelah bobot terbaru dihasilkan pada pelatihan terakhir, maka perlu dilihat apakah bobot tersebut dapat digeneralisasikan ke contoh lain atau hanya berfungsi baik pada training set. Oleh karena itu proses validasi perlu dilakukan untuk memodifikasi struktur neural network  hingga performanya cukup baik pada sampel yang baru. Setelah memiliki struktur neural network yang baik, kemudian struktur terssebut diterapkan pada test set untuk menguji performanya.\n",
        "\n"
      ],
      "metadata": {
        "id": "4A1wNhrJAADg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mengimport library Pytorch dan mensetting device**"
      ],
      "metadata": {
        "id": "VX5suYqURJVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device='cpu'\n",
        "#device='cuda'"
      ],
      "metadata": {
        "id": "yaejPjMFCV-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Percobaan 1**"
      ],
      "metadata": {
        "id": "BCepXBDODuwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mendefinisikan neural network dengan linear layer dan fungsi aktivasi non-linear**"
      ],
      "metadata": {
        "id": "y7TbG8PeSSg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinNet, self).__init__()\n",
        "        # Define the model.\n",
        "        self.layer1 = nn.Sequential(nn.Linear(in_features=2, out_features=2, bias=True))\n",
        "        #https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear\n",
        "        # Generate a fully connected linear neural network model, 1 layer, bias, linear activation function\n",
        "        # returns: Trainable object\n",
        "        #self.act = nn.LeakyReLU() #non-linear activation function\n",
        "        #self.act = nn.ReLU() #non-linear activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        #out = self.act(out) #comment out if not desired\n",
        "        return out"
      ],
      "metadata": {
        "id": "yTSv7fIdCo9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate data pelatihan dan data validasi**"
      ],
      "metadata": {
        "id": "TNC5j7JUS5go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input tensor, type torch tensor:\n",
        "#Indices: batch, sample, features or signal dimension. Here: 1 batch, 3 samples, signal dimension 2:\n",
        "\n",
        "#Training set:\n",
        "X=torch.tensor([[1., 2.], [2., 1.],[1., 1.]]).view(1,3,2) #adding the first dimension for the batch\n",
        "print(\"X.shape\", X.shape)\n",
        "\n",
        "#Target:\n",
        "Y=torch.tensor([[1., 0.], [0., 1.],[0., 0.]]).view(1,3,2)\n",
        "print(\"Y.shape\", Y.shape)\n",
        "\n",
        "#Validation set, to test generalization:\n",
        "Xval=torch.tensor([[0.5, 1.0], [1., 0.5],[0.5, 0.5]]).view(1,3,2)\n",
        "#Validation Target:\n",
        "Yval=torch.tensor([[1., 0.], [0., 1.],[0., 0.]]).view(1,3,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zySg1P7vCyKu",
        "outputId": "cc64d4dd-b85c-4388-a140-e5f62cdcf49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape torch.Size([1, 3, 2])\n",
            "Y.shape torch.Size([1, 3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Menginstansiasi model serta mendefinisikan loss function dan optimizer**\n",
        "\n",
        "Terdapat dua pilihan optimizer, yaitu Adam dan SGD (Stochastic Gradient Descent), kemudian dicoba manakah yang menghasilkan performa terbaik."
      ],
      "metadata": {
        "id": "T2HvYioKT75a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create network object:\n",
        "model = LinNet().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "print(\"Define loss function:\", loss_fn)\n",
        "#learning_rate = 1e-4\n",
        "#optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
        "print(\"Define optimizer:\", optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2bc3exlC2Vc",
        "outputId": "072c289c-30bf-416d-f3fc-a6ce5eb0985a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define loss function: MSELoss()\n",
            "Define optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.1\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer melakukan iterasi sebanyak 10000 iterasi**"
      ],
      "metadata": {
        "id": "CRPYotAQVObD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10000):\n",
        "    Ypred=model(X) #the model produces prediction output\n",
        "    loss=loss_fn(Ypred, Y) #prediction and target compared by loss\n",
        "    if epoch%100==0:\n",
        "        print(epoch, loss.item()) #print current loss value\n",
        "    optimizer.zero_grad() #optimizer sets previous gradients to zero\n",
        "    loss.backward() #optimizer computes new gradients\n",
        "    optimizer.step() #optimizer updates weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2NUjpR6C7LP",
        "outputId": "ba386cc6-d35e-428b-f693-86d6e3ee8038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.9978783130645752\n",
            "100 0.018016429618000984\n",
            "200 0.011064487509429455\n",
            "300 0.006851763930171728\n",
            "400 0.004243067000061274\n",
            "500 0.002627587178722024\n",
            "600 0.0016271768836304545\n",
            "700 0.0010076550533995032\n",
            "800 0.0006240036454983056\n",
            "900 0.00038642328581772745\n",
            "1000 0.00023929885355755687\n",
            "1100 0.00014818973431829363\n",
            "1200 9.176874300464988e-05\n",
            "1300 5.682918708771467e-05\n",
            "1400 3.5192388168070465e-05\n",
            "1500 2.1793506675749086e-05\n",
            "1600 1.3495777238858864e-05\n",
            "1700 8.357638762390707e-06\n",
            "1800 5.1754182095464785e-06\n",
            "1900 3.2048885714175412e-06\n",
            "2000 1.98468455891998e-06\n",
            "2100 1.2290516906432458e-06\n",
            "2200 7.611350270053663e-07\n",
            "2300 4.713199643902044e-07\n",
            "2400 2.918958728059806e-07\n",
            "2500 1.8077811603234295e-07\n",
            "2600 1.119428887363938e-07\n",
            "2700 6.933527885166768e-08\n",
            "2800 4.294554400985362e-08\n",
            "2900 2.6615678550001576e-08\n",
            "3000 1.6488913345824585e-08\n",
            "3100 1.0216187895650819e-08\n",
            "3200 6.333328972374375e-09\n",
            "3300 3.923443347986222e-09\n",
            "3400 2.4346178406631225e-09\n",
            "3500 1.511364700057527e-09\n",
            "3600 9.36699606768343e-10\n",
            "3700 5.803570957141346e-10\n",
            "3800 3.6213285148356533e-10\n",
            "3900 2.2361490437106113e-10\n",
            "4000 1.3921368224867336e-10\n",
            "4100 8.542085788709741e-11\n",
            "4200 5.47461347089051e-11\n",
            "4300 3.331320289068351e-11\n",
            "4400 2.0597449093551212e-11\n",
            "4500 1.3235634810371266e-11\n",
            "4600 8.632501831418171e-12\n",
            "4700 5.597300400950189e-12\n",
            "4800 4.545105177639064e-12\n",
            "4900 4.425496861332201e-12\n",
            "5000 4.425496861332201e-12\n",
            "5100 4.425496861332201e-12\n",
            "5200 4.425496861332201e-12\n",
            "5300 4.425496861332201e-12\n",
            "5400 4.425496861332201e-12\n",
            "5500 4.425496861332201e-12\n",
            "5600 4.425496861332201e-12\n",
            "5700 4.425496861332201e-12\n",
            "5800 4.425496861332201e-12\n",
            "5900 4.425496861332201e-12\n",
            "6000 4.425496861332201e-12\n",
            "6100 4.425496861332201e-12\n",
            "6200 4.425496861332201e-12\n",
            "6300 4.425496861332201e-12\n",
            "6400 4.425496861332201e-12\n",
            "6500 4.425496861332201e-12\n",
            "6600 4.425496861332201e-12\n",
            "6700 4.425496861332201e-12\n",
            "6800 4.425496861332201e-12\n",
            "6900 4.425496861332201e-12\n",
            "7000 4.425496861332201e-12\n",
            "7100 4.425496861332201e-12\n",
            "7200 4.425496861332201e-12\n",
            "7300 4.425496861332201e-12\n",
            "7400 4.425496861332201e-12\n",
            "7500 4.425496861332201e-12\n",
            "7600 4.425496861332201e-12\n",
            "7700 4.425496861332201e-12\n",
            "7800 4.425496861332201e-12\n",
            "7900 4.425496861332201e-12\n",
            "8000 4.425496861332201e-12\n",
            "8100 4.425496861332201e-12\n",
            "8200 4.425496861332201e-12\n",
            "8300 4.425496861332201e-12\n",
            "8400 4.425496861332201e-12\n",
            "8500 4.425496861332201e-12\n",
            "8600 4.425496861332201e-12\n",
            "8700 4.425496861332201e-12\n",
            "8800 4.425496861332201e-12\n",
            "8900 4.425496861332201e-12\n",
            "9000 4.425496861332201e-12\n",
            "9100 4.425496861332201e-12\n",
            "9200 4.425496861332201e-12\n",
            "9300 4.425496861332201e-12\n",
            "9400 4.425496861332201e-12\n",
            "9500 4.425496861332201e-12\n",
            "9600 4.425496861332201e-12\n",
            "9700 4.425496861332201e-12\n",
            "9800 4.425496861332201e-12\n",
            "9900 4.425496861332201e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mencetak hasil pada training set, validation set, dan bobot yang dihasilkan**"
      ],
      "metadata": {
        "id": "tzJ_GpV_VxEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred=model(X) # Make Predictions based on the obtained weights\n",
        "print(\"Ypred training set=\", Ypred)\n",
        "loss=loss_fn(Ypred, Y)\n",
        "print(\"Loss on trainig set:\", loss)\n",
        "Yvalpred=model(Xval) # Make Predictions based on the obtained weights\n",
        "print(\"Y validation set=\", Yvalpred)\n",
        "loss=loss_fn(Yvalpred, Yval)\n",
        "print(\"Loss on validation set:\", loss)\n",
        "weights = model.state_dict() #read obtained weights\n",
        "print(\"weights=\", weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C_bzWd3DCUV",
        "outputId": "16d54e6a-9144-4778-a9d1-731708f2783a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ypred training set= tensor([[[ 1.0000e+00, -8.3447e-07],\n",
            "         [-8.3447e-07,  1.0000e+00],\n",
            "         [ 3.2187e-06,  3.2187e-06]]], grad_fn=<ViewBackward0>)\n",
            "Loss on trainig set: tensor(4.4255e-12, grad_fn=<MseLossBackward0>)\n",
            "Y validation set= tensor([[[ 5.2452e-06, -4.9999e-01],\n",
            "         [-4.9999e-01,  5.2452e-06],\n",
            "         [-4.9999e-01, -4.9999e-01]]], grad_fn=<ViewBackward0>)\n",
            "Loss on validation set: tensor(0.5000, grad_fn=<MseLossBackward0>)\n",
            "weights= OrderedDict([('layer1.0.weight', tensor([[-4.0662e-06,  1.0000e+00],\n",
            "        [ 1.0000e+00, -4.0663e-06]])), ('layer1.0.bias', tensor([-1.0000, -1.0000]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Percobaan 2**"
      ],
      "metadata": {
        "id": "L13kojPJD1vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinNet, self).__init__()\n",
        "        # Define the model.\n",
        "        self.layer1 = nn.Sequential(nn.Linear(in_features=2, out_features=2, bias=True))\n",
        "        #https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear\n",
        "        # Generate a fully connected linear neural network model, 1 layer, bias, linear activation function\n",
        "        # returns: Trainable object\n",
        "        self.act = nn.LeakyReLU() #non-linear activation function\n",
        "        #self.act = nn.ReLU() #non-linear activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.act(out) #comment out if not desired\n",
        "        return out"
      ],
      "metadata": {
        "id": "rbr2J-_0D6_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create network object:\n",
        "model = LinNet().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "print(\"Define loss function:\", loss_fn)\n",
        "#learning_rate = 1e-4\n",
        "#optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
        "print(\"Define optimizer:\", optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8cC-pRQEAOg",
        "outputId": "e05d060f-62f0-4f0c-97b1-ac5356daa4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define loss function: MSELoss()\n",
            "Define optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.1\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10000):\n",
        "    Ypred=model(X) #the model produces prediction output\n",
        "    loss=loss_fn(Ypred, Y) #prediction and target compared by loss\n",
        "    if epoch%1000==0:\n",
        "        print(epoch, loss.item()) #print current loss value\n",
        "    optimizer.zero_grad() #optimizer sets previous gradients to zero\n",
        "    loss.backward() #optimizer computes new gradients\n",
        "    optimizer.step() #optimizer updates weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbSGlwe7EGqG",
        "outputId": "d099d118-06a2-4b12-ce83-c0c1be5769e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.8680738806724548\n",
            "1000 1.286183760385029e-05\n",
            "2000 1.2798948773706798e-05\n",
            "3000 1.2756426258420106e-05\n",
            "4000 1.2714059266727418e-05\n",
            "5000 1.2671794138441328e-05\n",
            "6000 1.262976093130419e-05\n",
            "7000 1.2587971468747128e-05\n",
            "8000 1.2546258403745014e-05\n",
            "9000 1.2504607184382621e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred=model(X) # Make Predictions based on the obtained weights\n",
        "print(\"Ypred training set=\", Ypred)\n",
        "loss=loss_fn(Ypred, Y)\n",
        "print(\"Loss on trainig set:\", loss)\n",
        "Yvalpred=model(Xval) # Make Predictions based on the obtained weights\n",
        "print(\"Y validation set=\", Yvalpred)\n",
        "loss=loss_fn(Yvalpred, Yval)\n",
        "print(\"Loss on validation set:\", loss)\n",
        "weights = model.state_dict() #read obtained weights\n",
        "print(\"weights=\", weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRgZI-KKEMMO",
        "outputId": "1fd7bfb2-7e4c-4346-8279-3719e27e5f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ypred training set= tensor([[[ 9.9996e-01, -4.1983e-03],\n",
            "         [-7.5578e-03,  9.9998e-01],\n",
            "         [ 1.5172e-04,  8.4698e-05]]], grad_fn=<LeakyReluBackward0>)\n",
            "Loss on trainig set: tensor(1.2463e-05, grad_fn=<MseLossBackward0>)\n",
            "Y validation set= tensor([[[ 0.3781, -0.0050],\n",
            "         [-0.0050,  0.2100],\n",
            "         [-0.0012, -0.0029]]], grad_fn=<LeakyReluBackward0>)\n",
            "Loss on validation set: tensor(0.1685, grad_fn=<MseLossBackward0>)\n",
            "weights= OrderedDict([('layer1.0.weight', tensor([[-0.7559,  0.9998],\n",
            "        [ 0.9999, -0.4199]])), ('layer1.0.bias', tensor([-0.2437, -0.5799]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Percobaan 3**"
      ],
      "metadata": {
        "id": "QxeMUSY-EO--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinNet, self).__init__()\n",
        "        # Define the model.\n",
        "        self.layer1 = nn.Sequential(nn.Linear(in_features=2, out_features=2, bias=True))\n",
        "        #https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear\n",
        "        # Generate a fully connected linear neural network model, 1 layer, bias, linear activation function\n",
        "        # returns: Trainable object\n",
        "        #self.act = nn.LeakyReLU() #non-linear activation function\n",
        "        self.act = nn.ReLU() #non-linear activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.act(out) #comment out if not desired\n",
        "        return out"
      ],
      "metadata": {
        "id": "idBa9vaTESqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create network object:\n",
        "model = LinNet().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "print(\"Define loss function:\", loss_fn)\n",
        "#learning_rate = 1e-4\n",
        "#optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
        "print(\"Define optimizer:\", optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRB1vuEdEXYh",
        "outputId": "9131e10b-3ce0-4d1b-fc10-220c2be96624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define loss function: MSELoss()\n",
            "Define optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    lr: 0.1\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10000):\n",
        "    Ypred=model(X) #the model produces prediction output\n",
        "    loss=loss_fn(Ypred, Y) #prediction and target compared by loss\n",
        "    if epoch%1000==0:\n",
        "        print(epoch, loss.item()) #print current loss value\n",
        "    optimizer.zero_grad() #optimizer sets previous gradients to zero\n",
        "    loss.backward() #optimizer computes new gradients\n",
        "    optimizer.step() #optimizer updates weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auPFNORAEbfK",
        "outputId": "1a21d365-5053-4c9a-9b78-9a065c751b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.3333711326122284\n",
            "1000 0.3333333432674408\n",
            "2000 0.3333333432674408\n",
            "3000 0.3333333432674408\n",
            "4000 0.3333333432674408\n",
            "5000 0.3333333432674408\n",
            "6000 0.3333333432674408\n",
            "7000 0.3333333432674408\n",
            "8000 0.3333333432674408\n",
            "9000 0.3333333432674408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred=model(X) # Make Predictions based on the obtained weights\n",
        "print(\"Ypred training set=\", Ypred)\n",
        "loss=loss_fn(Ypred, Y)\n",
        "print(\"Loss on trainig set:\", loss)\n",
        "Yvalpred=model(Xval) # Make Predictions based on the obtained weights\n",
        "print(\"Y validation set=\", Yvalpred)\n",
        "loss=loss_fn(Yvalpred, Yval)\n",
        "print(\"Loss on validation set:\", loss)\n",
        "weights = model.state_dict() #read obtained weights\n",
        "print(\"weights=\", weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVZcI6BkEgNX",
        "outputId": "4a2e05e5-9002-47cd-e59a-e7d1b41b7750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ypred training set= tensor([[[0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00],\n",
            "         [8.9407e-08, 0.0000e+00]]], grad_fn=<ReluBackward0>)\n",
            "Loss on trainig set: tensor(0.3333, grad_fn=<MseLossBackward0>)\n",
            "Y validation set= tensor([[[0.1863, 0.0000],\n",
            "         [0.0323, 0.0000],\n",
            "         [0.2186, 0.0000]]], grad_fn=<ReluBackward0>)\n",
            "Loss on validation set: tensor(0.2852, grad_fn=<MseLossBackward0>)\n",
            "weights= OrderedDict([('layer1.0.weight', tensor([[-0.3725, -0.0646],\n",
            "        [-0.4424, -0.4147]])), ('layer1.0.bias', tensor([ 0.4371, -0.6121]))])\n"
          ]
        }
      ]
    }
  ]
}