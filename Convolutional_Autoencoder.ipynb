{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Convolutional Autoencoder**"
      ],
      "metadata": {
        "id": "7Zg-wgP0ZsSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Suatu autoencoder adalah *neural network* dengan *unsupervised learning* yang artinya tidak perlu menyediakan fungsi target dimana hanya terdapat  *training se*t yang juga merupakan *target set*.\n",
        "- Suatu *convolutional autoencoder* menggunakan *convolutional neural network*.\n",
        "- Suatu autoencoder memetakan sinyal *input* ke representasi dimensional yang lebih rendah menggunkan bagian *encoder*-nya dan memetakan representasi dimensional yang lebih rendah ke representasi dimensional yang lebih tinggi menggunakan bagian *decoder*-nya.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vgjsJsUnaUL_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1m24pweSWAw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if sys.version_info[0] < 3:\n",
        "   # untuk Python 2\n",
        "   import cPickle as pickle\n",
        "else:\n",
        "   # untuk Python 3\n",
        "   import pickle\n",
        "\n",
        "# konfigurasi perangkat\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device=\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOW66vIRWjiD",
        "outputId": "d2ff7fc9-d1df-4bec-c637-e43e8b96a19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device= cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def signal2pytorch(x):\n",
        "    #Function to convert a signal vector x, like a mono audio signal, into a 3-d Tensor that conv1d of Pytorch expects,\n",
        "    #https://pytorch.org/docs/stable/nn.html\n",
        "    #Argument x: a 1-d signal as numpy array\n",
        "    #input x[batch,sample]\n",
        "    #output: 3-d Tensor X for conv1d input.\n",
        "    #for conv1d Input: (N,Cin,Lin), Cin: numer of input channels (e.g. for stereo), Lin: length of signal, N: number of Batches (signals)\n",
        "    X = np.expand_dims(x, axis=0)  #add channels dimension (here only 1 channel)\n",
        "    if len(x.shape)==1: #mono:\n",
        "        X = np.expand_dims(X, axis=0)  #add batch dimension (here only 1 batch)\n",
        "    X=torch.from_numpy(X)\n",
        "    X=X.type(torch.Tensor)\n",
        "    X=X.permute(1,0,2)  #make batch dimension first\n",
        "    return X"
      ],
      "metadata": {
        "id": "Q1PnjsiKWl9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fungsi - aktivasi: tanh\n",
        "- Stride - atau *downsampling factor* N :1024\n",
        "- Ukuran - kernel fiter: 2N"
      ],
      "metadata": {
        "id": "0_HSzMYlkBPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convautoenc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Convautoenc, self).__init__()\n",
        "        #Analisis Filterbank dengan downsampling o N=1024, panjang filter 2N, tetapi hanya N/2 outputs:\n",
        "        self.conv1=nn.Conv1d(in_channels=1, out_channels=32, kernel_size=2048, stride=1024, padding=1023, bias=True) #Padding for 'same' filters (kernel_size/2-1)\n",
        "\n",
        "        #Synthesis filter bank:\n",
        "        self.synconv1=nn.ConvTranspose1d(in_channels=32, out_channels=1, kernel_size=2048, stride=1024, padding=1023, bias=True)\n",
        "\n",
        "    def encoder(self, x):\n",
        "        #Analysis:\n",
        "        x = self.conv1(x)\n",
        "        y = torch.tanh(x)\n",
        "        return y\n",
        "\n",
        "    def decoder(self, y):\n",
        "        #Synthesis:\n",
        "        xrek= self.synconv1(y)\n",
        "        return xrek\n",
        "\n",
        "    def forward(self, x):\n",
        "        y=self.encoder(x)\n",
        "        #y=torch.round(y/0.125)*0.125\n",
        "        xrek=self.decoder(y)\n",
        "        return xrek"
      ],
      "metadata": {
        "id": "jJ4Vewu-Wm67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alternative: speech:\n",
        "batch=1\n",
        "audio, samplerate = librosa.load(\"./audio/ACDC - Back In Black Intro.wav\", mono=False, sr=None, offset=6)\n",
        "audio[0,:]/=np.abs(audio[0,:]).max()\n",
        "audio[1,:]/=np.abs(audio[1,:]).max()\n",
        "X_train=signal2pytorch(audio[0,:]).to(device) #Convert to pytorch format, batch is first dimension\n",
        "X_test=signal2pytorch(audio[1,:]).to(device) #Convert to pytorch format, batch is first dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "1Njsa7uVWprQ",
        "outputId": "6b2feec1-eb8b-414b-fef4-7ca2901352d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-4ad7d0d126a2>:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, samplerate = librosa.load(\"./audio/ACDC - Back In Black Intro.wav\", mono=False, sr=None, offset=6)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './audio/ACDC - Back In Black Intro.wav'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__soundfile_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Otherwise, create the soundfile object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    657\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLibsndfileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error opening {0!r}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLibsndfileError\u001b[0m: Error opening './audio/ACDC - Back In Black Intro.wav': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4ad7d0d126a2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#alternative: speech:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./audio/ACDC - Back In Black Intro.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 )\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-119>\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/util/decorators.py\u001b[0m in \u001b[0;36m__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Would be 2, but the decorator adds a level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# If the input was not an audioread object, try to open it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \"\"\"\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './audio/ACDC - Back In Black Intro.wav'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generate Model:\")\n",
        "model = Convautoenc().to(device)\n",
        "print('Total number of parameters: %i' % (sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
        "print(\"Def. loss function:\")\n",
        "loss_fn = nn.MSELoss()  #MSE\n",
        "#loss_fn = nn.L1Loss()\n",
        "\n",
        "Ypred=model(X_train)\n",
        "\n",
        "#Ypred=Ypred.detach()\n",
        "outputlen=len(Ypred[0,0,:]) #length of the signal at the output of the network.\n",
        "print(\"outputlen=\", outputlen)\n",
        "\n",
        "Y=X_train[:,:,:outputlen]  #the target signal with same length as model output\n",
        "\n",
        "print(\"Input X.shape=\", X_train.shape )\n",
        "print(\"Target Y.shape=\", Y.shape)\n",
        "print(\"Target Y=\", Y)\n",
        "#print(\"max(max(Y))=\", max(max(max(Y))))\n",
        "#print(\"min(min(Y))=\", min(min(min(Y))))\n",
        "print(\"Y.type()=\", Y.type())"
      ],
      "metadata": {
        "id": "2nQ6OilyW2Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, betas=(0.9, 0.999))\n",
        "\"\"\"\n",
        "try:\n",
        "    checkpoint = torch.load(\"audio_autoenc.torch\",map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "except IOError:\n",
        "    print(\"fresh start\")\n",
        "\"\"\";\n",
        "\n",
        "#optimrandomdir_pytorch.optimizer(model, loss_fn, X, Ypred, iterations=300, startingscale=1.0, endscale=0.0)\n",
        "Ypred=model(X_train)\n",
        "#Ypred=Ypred.detach()\n",
        "print(\"Ypred=\", Ypred)\n",
        "\n",
        "#randdir=True # True for optimization of random direction, False for pytorch optimization\n",
        "randdir=False\n",
        "\n",
        "if randdir==True:\n",
        "#optimization of weights using method of random directions:\n",
        "    optimrandomdir_pytorch.optimizer(model, loss_fn, X_train, Y, iterations=100000, startingscale=0.25, endscale=0.0)\n",
        "    #--End optimization of random directions------------------------\n",
        "else:\n",
        "    for epoch in range(10000):\n",
        "        Ypred=model(X_train)\n",
        "        #print(\"Ypred.shape=\", Ypred.shape)\n",
        "        #loss wants batch in the beginning! (Batch, Classes,...)\n",
        "        #Ypredp=Ypred.permute(1,2,0)\n",
        "        #Yp=Y.permute(1,0)\n",
        "        #print(\"Ypredp.shape=\", Ypredp.shape, \"Yp.shape=\", Yp.shape )\n",
        "        loss=loss_fn(Ypred, Y)\n",
        "        if epoch%10==0:\n",
        "            print(epoch, loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "UMCijGIQW7je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "torch.save({#'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()}, \"audio_autoenc.torch\")\n",
        "\"\"\"\n",
        "\n",
        "ww = model.state_dict()   #read obtained weights\n",
        "print(\"ww=\", ww)\n",
        "#Plot obtained weights:\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(np.transpose(np.array(ww['conv1.weight'][0:1,0,:])))\n",
        "plt.plot(np.transpose(np.array(ww['synconv1.weight'][0:1,0,:])))\n",
        "plt.legend(('Encoder Analysis filter 0', 'Decoder Filter 0'))\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Value')\n",
        "plt.title('The Encoder and Decoder Filter Coefficients')\n",
        "plt.grid()\n",
        "\n",
        "#Test on training set:\n",
        "predictions=model(X_train).cpu() # Make Predictions based on the obtained weights, on training set\n",
        "predictions=predictions.detach()\n",
        "predictions=np.array(predictions)\n",
        "Y=np.array(Y) #target\n",
        "#print(\"Y=\",Y)\n",
        "print(\"predictions.shape=\", predictions.shape)\n",
        "#convert to numpy:\n",
        "#https://discuss.pytorch.org/t/how-to-transform-variable-into-numpy/104/2\n",
        "#Plot target signal and output of autoencoder:\n",
        "plt.figure(figsize=(10,6))\n",
        "for b in range(batch):\n",
        "    plt.plot(np.array(Y[b,0,:]))\n",
        "    plt.plot(predictions[b,0,:])\n",
        "    plt.legend(('Target','Predicted'))\n",
        "    plt.title('The Target and Predicted Signal, batch '+str(b))\n",
        "    plt.xlabel('Sample')\n",
        "    plt.grid()\n",
        "xrek=predictions[:,0,:]  #remove unnecessary dimension for playback\n",
        "#xrek=np.transpose(xrek)\n",
        "#xrek=np.clip(xrek, -1.0,1.0)"
      ],
      "metadata": {
        "id": "_Xb6Xv89XCJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test on Verification set:\n",
        "predictions=model(X_test).cpu() # Make Predictions based on the obtained weights, on verification set\n",
        "predictions=predictions.detach()\n",
        "predictions=np.array(predictions)\n",
        "plt.figure(figsize=(10,6))\n",
        "for b in range(batch):\n",
        "    plt.plot(np.array(X_test[b,0,:]))\n",
        "    plt.plot(predictions[b,0,:])\n",
        "    plt.legend(('Original','Predicted'))\n",
        "    plt.title('The Original and Predicted Signal, batch '+str(b))\n",
        "    plt.xlabel('Sample')\n",
        "    plt.grid()\n",
        "xrek=predictions[:,0,:]"
      ],
      "metadata": {
        "id": "DfTXf4r4XE2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Effect dari Signal Shifts**"
      ],
      "metadata": {
        "id": "HGWsch1QXHkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test on shifted input:\n",
        "X_train_shifted_100 = nn.ConstantPad1d(100, 0)(X_train)\n",
        "predictions=model(X_train_shifted_100).cpu() # Make Predictions based on the obtained weights, on verification set\n",
        "predictions=predictions.detach()\n",
        "predictions=np.array(predictions)\n",
        "xrek=predictions[:,0,:]"
      ],
      "metadata": {
        "id": "NhlvigW9XWCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(ipd.Audio(xrek, rate=samplerate));"
      ],
      "metadata": {
        "id": "th_q_Ey-Xa_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test on 1024 samples shifted test set (shift identical to the stride size)\n",
        "X_train_shifted_1024 = nn.ConstantPad1d(1024, 0)(X_train)\n",
        "predictions=model(X_train_shifted_1024).cpu() # Make Predictions based on the obtained weights, on verification set\n",
        "predictions=predictions.detach()\n",
        "predictions=np.array(predictions)\n",
        "xrek=predictions[:,0,:]"
      ],
      "metadata": {
        "id": "56MxWcVwXcw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(ipd.Audio(xrek, rate=samplerate));"
      ],
      "metadata": {
        "id": "u55qgCQRXfMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}